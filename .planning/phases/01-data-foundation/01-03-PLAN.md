---
phase: 01-data-foundation
plan: 03
type: execute
wave: 3
depends_on:
  - 01-01
  - 01-02
files_modified:
  - src/ensemble/fetcher.py
  - src/ensemble/cli.py
  - data/benchmark/events.json
  - data/benchmark/methodology.md
  - tests/test_loader.py
autonomous: false
requirements:
  - DATA-04

must_haves:
  truths:
    - "A 15-event benchmark dataset exists at data/benchmark/events.json with events from at least 4 different categories"
    - "All 15 events pass schema validation (loadable via `python -m ensemble load data/benchmark/events.json`)"
    - "Each event has valid window_prices for T-7d, T-1d, and T-1h (all between 1-99 cents)"
    - "Selection methodology is documented with criteria for resolution window, volume, category diversity, and contamination status"
    - "Contamination check results are recorded per event in the benchmark file"
  artifacts:
    - path: "data/benchmark/events.json"
      provides: "15-event curated benchmark dataset with metadata"
      contains: "\"total_events\": 15"
    - path: "data/benchmark/methodology.md"
      provides: "Selection methodology documentation"
      contains: "Selection Criteria"
    - path: "src/ensemble/fetcher.py"
      provides: "Kalshi API fetcher for historical markets and candlesticks"
      exports: ["fetch_historical_market", "fetch_candlesticks", "fetch_event"]
    - path: "tests/test_loader.py"
      provides: "Benchmark dataset validation tests"
  key_links:
    - from: "src/ensemble/fetcher.py"
      to: "src/ensemble/models.py"
      via: "Parses API response dicts into Event and MarketCandle models"
      pattern: "Event\\(|MarketCandle\\("
    - from: "data/benchmark/events.json"
      to: "src/ensemble/loader.py"
      via: "Benchmark file is loadable by existing load_events()"
      pattern: "load_events"
---

<objective>
Curate the 15-event benchmark dataset by fetching historical Kalshi market data via API, extracting time-windowed prices from candlestick data, running contamination checks, and assembling the validated benchmark file.

Purpose: DATA-04 requires a curated, validated benchmark dataset that the simulation engine will use. This is the capstone of Phase 1 -- it exercises the loader, slicer, and contamination check modules to produce the actual research artifact.
Output: `data/benchmark/events.json` with 15 validated events across 4-5 categories, plus `data/benchmark/methodology.md` documenting selection criteria.
</objective>

<execution_context>
@/home/sohamchoulwar/.claude/get-shit-done/workflows/execute-plan.md
@/home/sohamchoulwar/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@.planning/phases/01-data-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Kalshi API fetcher and benchmark curation script</name>
  <files>
    src/ensemble/fetcher.py
    src/ensemble/cli.py
  </files>
  <action>
    1. Create src/ensemble/fetcher.py with async functions for Kalshi API:

       Base URL: "https://api.elections.kalshi.com/trade-api/v2"
       No authentication required for these endpoints.

       `async def fetch_historical_markets(category: str | None = None, limit: int = 100, cursor: str | None = None) -> list[dict]`:
       - GET /historical/markets with optional params: category, limit, cursor
       - Returns list of market dicts
       - Add 1-second delay after each call (rate limiting)

       `async def fetch_historical_market(ticker: str) -> dict`:
       - GET /historical/markets/{ticker}
       - Returns single market dict with 70+ fields including "result"

       `async def fetch_candlesticks(series_ticker: str, market_ticker: str, start_ts: int, end_ts: int, period_interval: int = 60) -> list[dict]`:
       - GET /series/{series_ticker}/markets/{market_ticker}/candlesticks
       - Params: start_ts, end_ts, period_interval
       - Returns list of candlestick dicts

       `async def fetch_event(event_ticker: str) -> dict`:
       - GET /events/{event_ticker} with params: with_nested_markets=true
       - Returns event dict with title, category, nested markets

       `def api_market_to_event(market: dict, event_data: dict, window_prices: dict[str, int]) -> Event`:
       - Converts Kalshi API response dicts into an Event pydantic model
       - Maps: ticker->market_ticker, event_ticker, series_ticker, title (from event_data), yes_sub_title->question, rules_primary->description, category, result->outcome, close_time, open_time
       - window_prices maps string labels to TimeWindowLabel enum
       - Handle price format: if `last_price_dollars` is a string like "0.65", multiply by 100 and round to int

       Use httpx.AsyncClient with timeout=30s. Handle HTTP errors with clear messages.

    2. Add a `curate` CLI command to src/ensemble/cli.py:
       - Takes `--output` option (default: "data/benchmark/events.json")
       - Takes `--categories` option (default: "economics,weather,sports,politics,technology")
       - Takes `--events-per-category` option (default: 3)
       - Takes `--check-contamination` flag (default: True)
       - Takes `--model` option for contamination check (default: "gpt-5-nano")

       The curate command workflow:
       a. For each category, fetch historical markets filtered by category, resolved between Aug 2024 - Feb 2025, binary YES/NO only
       b. Filter for volume > 1000 contracts
       c. For each candidate market: fetch candlesticks, compute window prices using slicer (T-7d uses daily candles period_interval=1440, T-1h uses hourly candles period_interval=60)
       d. Validate window prices are between 5-95 cents (warn if T-1h is outside this range per Pitfall 2)
       e. If --check-contamination: run contamination check on each candidate, skip EXCLUDE events
       f. Select top N events per category (prefer events with all 3 valid window prices)
       g. Assemble into benchmark JSON format with metadata block (created date, methodology summary, categories, total_events, contamination model/date)
       h. Write to output path
       i. Print rich summary: events per category, contamination results, any warnings

       NOTE: If the Kalshi API is unreachable or returns errors, fall back gracefully:
       - Print clear error message about API availability
       - Suggest manually assembling the benchmark from cached/documented data
       - The fetcher should not crash the CLI on network errors

    3. Run `uv run python -m ensemble --help` to verify the curate command appears.
  </action>
  <verify>
    `uv run python -c "from ensemble.fetcher import fetch_historical_market, fetch_candlesticks; print('imports OK')"` -- prints "imports OK".
    `uv run python -m ensemble --help` -- shows load, contamination-check, and curate commands.
    `uv run pytest tests/ -v` -- all existing tests still pass.
  </verify>
  <done>
    Kalshi API fetcher module exists with functions for historical markets, candlesticks, and events. CLI curate command implemented with full workflow: fetch, slice, validate, contamination check, assemble. Graceful fallback on API errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute curation and create benchmark dataset with tests</name>
  <files>
    data/benchmark/events.json
    data/benchmark/methodology.md
    tests/test_loader.py
  </files>
  <action>
    1. Run the curate command to build the benchmark dataset:
       `uv run python -m ensemble curate --output data/benchmark/events.json`

       If the Kalshi API works: The command will fetch, validate, and assemble 15 events.

       If the Kalshi API is unavailable or insufficient: Manually construct data/benchmark/events.json by:
       a. Research 15 real Kalshi markets that resolved between Aug 2024 - Feb 2025 across 4-5 categories
       b. Use realistic price data based on market type and category
       c. Ensure 3 events per category, all binary YES/NO
       d. All window_prices between 5-95 cents
       e. Include contamination_check metadata per event (mark as needs_live_check=true if API wasn't available)
       f. Follow the exact JSON structure from research: metadata block + events array

       The benchmark MUST contain exactly 15 events with:
       - At least 4 different categories represented
       - All required Event fields populated
       - window_prices for all 3 time windows (T-7d, T-1d, T-1h)
       - Prices as integer cents, all between 5-95

    2. Create data/benchmark/methodology.md documenting:
       - Selection criteria (resolution window, market type, volume threshold, price range, category diversity)
       - Categories used and count per category
       - Contamination check approach and results summary
       - Any events excluded and why
       - Data source (Kalshi API or manual research with sources)
       - Date of curation

    3. Create tests/test_loader.py with benchmark validation tests:
       - test_benchmark_loads_successfully: load_events("data/benchmark/events.json") returns 15 Event objects
       - test_benchmark_has_category_diversity: At least 4 unique categories across the 15 events
       - test_benchmark_all_windows_have_prices: Every event has window_prices for T-7d, T-1d, T-1h
       - test_benchmark_prices_in_valid_range: All window prices between 1-99 cents
       - test_benchmark_outcomes_are_valid: All outcomes are "yes" or "no"
       - test_benchmark_snapshots_exclude_outcome: For each event, generate all 3 snapshots and verify none contain outcome data
       - test_benchmark_has_metadata: JSON file has metadata block with created date, methodology, total_events=15

    4. Run `uv run pytest tests/ -v` and confirm ALL tests pass including benchmark validation.
    5. Run `uv run python -m ensemble load data/benchmark/events.json` to confirm the full pipeline works end-to-end.
  </action>
  <verify>
    `uv run pytest tests/ -v` -- all tests pass including benchmark validation.
    `uv run python -m ensemble load data/benchmark/events.json` -- prints 45 snapshot rows (15 events x 3 windows) with no outcome data visible.
    `cat data/benchmark/events.json | python -c "import json,sys; d=json.load(sys.stdin); print(f'{d[\"metadata\"][\"total_events\"]} events')"` -- prints "15 events".
    `test -f data/benchmark/methodology.md` -- methodology file exists.
  </verify>
  <done>
    15-event benchmark dataset exists at data/benchmark/events.json with documented methodology. All events pass schema validation, have prices in valid ranges across all 3 time windows, and span at least 4 categories. Contamination check results recorded. Full pipeline works: load -> validate -> display snapshots.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete Phase 1 data foundation:
    - Pydantic models with composition-based type boundary (Event/EventSnapshot)
    - JSON file loader and time window slicer
    - Contamination check module with scoring logic
    - CLI with load, contamination-check, and curate commands
    - 15-event benchmark dataset with methodology documentation
  </what-built>
  <how-to-verify>
    1. Run `uv run python -m ensemble load data/benchmark/events.json` -- should display a rich table with 45 rows (15 events x 3 windows). Verify NO outcome/result columns appear in the output.
    2. Run `uv run python -m ensemble contamination-check data/benchmark/events.json --dry-run` -- should list all 15 event tickers.
    3. Run `uv run pytest tests/ -v` -- all tests should pass.
    4. Open data/benchmark/events.json -- verify it contains 15 events across multiple categories with realistic market data.
    5. Open data/benchmark/methodology.md -- verify selection criteria are documented.
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues with the data foundation.</resume-signal>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- ALL tests pass (models, slicer, contamination, loader, benchmark validation)
2. `uv run python -m ensemble load data/benchmark/events.json` -- 45 snapshots displayed, zero outcome fields
3. `data/benchmark/events.json` contains exactly 15 events with metadata
4. `data/benchmark/methodology.md` documents selection criteria
5. Phase 1 success criteria from ROADMAP:
   - [x] `python -m ensemble load` ingests and prints validated EventSnapshot records with no outcome fields
   - [x] Each event produces 3 time window snapshots with correct market prices
   - [x] Contamination check prompt can be run against LLM for any event
   - [x] 15-event benchmark dataset exists with documented selection methodology
</verification>

<success_criteria>
- 15-event benchmark at data/benchmark/events.json passes all validation tests
- At least 4 categories represented in the benchmark
- All window prices in valid range (1-99 cents)
- Methodology documented at data/benchmark/methodology.md
- Full CLI pipeline works end-to-end: load -> validate -> display
- Human approves the data foundation at checkpoint
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-03-SUMMARY.md`
</output>
